\documentclass[10pt,a4paper,twocolumn]{article}
\usepackage{graphicx,amsmath,amssymb,bm,xcolor,listings, mathtools} %navigator
%\RestyleAlgo{ruled}
%\soulregister\cite7
%\soulregister\ref7

\definecolor{rr}{rgb}{.8,0,0}
\definecolor{rr2}{rgb}{1,.6,.6}
\definecolor{gg}{rgb}{0,.6,0}
\definecolor{bb}{rgb}{0,0,.8}
\definecolor{codegreen}{rgb}{0,0.6,0}
\definecolor{codegray}{rgb}{0.5,0.5,0.5}
\definecolor{codepurple}{rgb}{0.58,0,0.82}
\definecolor{backcolour}{rgb}{0.95,0.95,0.92}

%\usepackage{sectsty}
%\sectionfont{\color{rr}\large}
%\subsectionfont{\color{rr}\normalsize}
%\subsubsectionfont{\color{rr}\normalsize}
%\makeatletter
%\renewcommand\@seccntformat[1]{%
%	\colorbox{rr}{\textcolor{white}{\csname the#1\endcsname}}%
%	\,
%}
\makeatother

\lstdefinestyle{mystyle}{
	backgroundcolor=\color{backcolour},   
	commentstyle=\color{codegreen},
	keywordstyle=\color{magenta},
	numberstyle=\tiny\color{codegray},
	stringstyle=\color{codepurple},
	basicstyle=\ttfamily\scriptsize,
	breakatwhitespace=false,         
	breaklines=true,       
	captionpos=b,                    
	keepspaces=true,                                     
	numbersep=5pt,                  
	numbers=left,                    
	numbersep=5pt,                  
	showspaces=false,                
	showstringspaces=false,
	showtabs=false,                  
	tabsize=2
}
\lstset{style=mystyle}

%\newcommand{\hlm}[1]{\colorbox{yellow}{$\displaystyle #1$}}

\usepackage{geometry}
\geometry{a4paper, right=1.5cm, bottom=2.0cm, textwidth=18cm, textheight=26.0cm, marginparsep=7pt, marginparwidth=.6in}
\setlength{\columnsep}{8mm}
%\setlength\parindent{0in}

\newcommand{\trsp}{{\scriptscriptstyle\top}}
\newcommand{\psin}{{\dagger}}
\newcommand{\tp}[1]{\text{\tiny#1}}
\newcommand{\ty}[1]{{\scriptscriptstyle{#1}}}
\newcommand{\diag}{\mathrm{diag}}
\newcommand{\tr}{\mathrm{tr}}
\newcommand{\cov}{\mathrm{cov}}
\newcommand{\tmin}{{\scriptscriptstyle\min}}
\newcommand{\tmax}{{\scriptscriptstyle\max}}
%\newcommand{\filename}[1]{{\raggedleft\colorbox{rr2}{{\color{white}\texttt{#1}}}\\[2mm]}}
\newcommand{\filename}[1]{\colorbox{rr2}{\color{white}\texttt{#1}}}

\usepackage{hyperref}
\hypersetup{
	colorlinks=true,
	linkcolor=blue,
	filecolor=magenta,      
	urlcolor=cyan,
	pdftitle={Overleaf Example},
	pdfpagemode=FullScreen,
}

\title{\huge Gaussian Process Flavours}
\author{Davide Carminati}
\date{}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{document}
	
\maketitle


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Kernel Eigenvalue Decomposition}\label{kernelDec}
The kernel series approximation relies on \textit{Mercer's expansion}:
\begin{equation}\label{eq:mercer}
	K_{(\mathbf{x}, \mathbf{x'})} = \sum_{n=0}^{\infty} \lambda_n \varphi_n(\mathbf{x}) \varphi_n(\mathbf{x'})
\end{equation}
in which $\lambda_n$ and $\varphi_n$ are the \textit{n-th} eingenvalue and eigenfunction, respectively.
Using a finite series approximation, a Gaussian Process kernel can be written as:
\begin{equation}\label{eq:mercerLim}
	K_{(\mathbf{x}, \mathbf{x'})} \approx \sum_{n=0}^{m} \lambda_n \varphi_n(\mathbf{x}) \varphi_n(\mathbf{x'})
\end{equation}
where $m$ is the number of eigenvalues considered. $m$ is much smaller than the number of datapoints $\mathbf{x}_1 \ldots \mathbf{x}_N$. The advantage is that we consider a much smaller kernel $K_{approx} \in \mathrm{R}^{m \times m}$ to be inverted, instead of the classical $K \in \mathrm{R}^{N \times N}$.
\subsection{Squared Exponential kernel decomposition}
The \textit{Squared Exponential} kernel is defined as:
\begin{equation}
	K_{(x,x')} = \exp^ {-\varepsilon^2 (x - x')^2 }
\end{equation}
Setting:
\small
\begin{align}
	 \beta &= \left(1 + \left(\frac{2\varepsilon}{\alpha}\right)^2\right)^\frac{1}{4}, && \gamma_n = \sqrt{\frac{\beta}{2^{n-1}\Gamma_{(n)}}}, && \delta^2 = \frac{\alpha}{2}(\beta^2-1),
\end{align}
\normalsize
the \textit{Squared Exponential} kernel can be decomposed using \autoref{eq:mercer} and one obtains the eigenfunctions:
% \cite{fasshauer2011positive}:
\begin{equation}\label{eq:SEeigfun}
	\varphi_n(x) = \gamma_n \exp^{-\delta^2 x^2} H_{n-1}(\alpha \beta x)
\end{equation}
where $H_{n-1}$ is the \textit{classical} Hermite polynomial of degree $n-1$.
Their corresponding \textit{eigenvalues} are defined as:
\begin{equation}
	\lambda_n = \sqrt{\frac{\alpha^2}{\alpha^2 + \delta^2 + \varepsilon^2}} \left(\frac{\varepsilon^2}{\alpha^2 + \delta^2 + \varepsilon^2} \right)^{n-1}
\end{equation}
\subsection{Multivariate expansion}
The $d$-variate squared exponential kernel is defined as:
\begin{equation}
	K_{(\mathbf{x}, \mathbf{x'})} = \exp^{-\varepsilon_1^2\left(x_1 - x'_1\right)^2  - \ldots - \varepsilon_d^2\left(x_d - x'_d\right)^2}
\end{equation}
For $d$-variate kernels it holds the following expansion:
\begin{equation}
	K_{(\mathbf{x},\mathbf{x'})} =  \sum_{\mathbf{n}\in \mathbb{N}^d} \lambda_{\mathbf{n}} \varphi_{\mathbf{n}}(x) \varphi_{\mathbf{n}}(x')
	\end{equation}
where $\mathbf{n}$ is the set of all $n^d$ combination of the considered number of eigenvalues.
The eigenvalues $\lambda_{\mathbf{n}}$ and eigenfunctions $\varphi_{\mathbf{n}}(\mathbf{x})$ are defined as:
% in \cite{fasshauer2012stable}:
\begin{align}
	\lambda_{\mathbf{n}} &= \prod_{j=1}^{d} \lambda_{n_j} \\
	\varphi_{\mathbf{n}}(\mathbf{x}) &= \prod_{j=1}^{d} \varphi_{n_{j}}(x_j)
\end{align}
where $d$ is the number of dimensions. 
%For the 2D case, $\mathbf{x} \in \mathrm{R}^{N \times d}$, $ \varphi_{\mathbf{n}} \in \mathrm{R}^{N}$.
	
\section{Gaussian Process Implicit Surface}\label{GPIS}
Gaussian Process Implicit Surface (GPIS) allows the modeling of obstacles in environments by imposing a suitable $y$ value to the regression problem. In particular, each $\mathbf{x}$ point has a value
\begin{equation}
	y = 
	\begin{cases}
		-1 & \text{outside obstacle}\\
		0 & \text{on the edge}\\
		1 & \text{inside obstacle}
	\end{cases}
\end{equation}
Given a train dataset $ \{ (\mathbf{x}_i, y_i) \}_{i=1}^N$, with $\mathbf{x}_i \in \mathbf{X}$ and $y_i \in \mathbf{y}$ and a test dataset $\{ (\mathbf{x}_{*i}, y_{*i}) \}_{i=1}^M$, with $\mathbf{x}_{*i} \in \mathbf{X_*}$, the kernels are written as:
\small
\begin{align}
	\mathbf{K} = K(\mathbf{X},\mathbf{X}), && \mathbf{k_*} = K(\mathbf{X_*},\mathbf{X}), && \mathbf{k_{**}} = K(\mathbf{X_*},\mathbf{X_*})
\end{align}
\normalsize
The GP regression problem is as follows:
\begin{equation}\label{eq:GPregr}
	\begin{aligned}
		\mathbf{\bar{f}_*} &= \mathbf{k}_*[\mathbf{K} + \sigma_N^2 \mathrm{I}]^{-1}\mathbf{y}\\    
		\cov(\mathbf{f}_*) &= \mathbf{k}_{**} - \mathbf{k}_*[\mathbf{K} + \sigma_N^2 \mathrm{I}]^{-1}\mathbf{k}_*^\trsp
	\end{aligned}
\end{equation}
where $\mathbf{\bar{f}_*}$ is the prediction on the test dataset and $\cov(\mathbf{f}_*)$ is the associated uncertainty on the prediction.
In the classic GPIS algorithms, the \textit{Square Exponential} kernel (\autoref{eq:sekernel}) and the \textit{Thin Plate} kernel (\autoref{eq:tpkernel}) are used.
\begin{equation}\label{eq:sekernel}
	K_{SE(\mathbf{x}, \mathbf{x'})}  = \sigma^2 \exp \left( -\frac{\| \mathbf{x} - \mathbf{x'} \|}{2 l^2} \right)
\end{equation}
\begin{equation}\label{eq:tpkernel}
	K_{(\mathbf{x}, \mathbf{x'})} = 2\|\mathbf{x} - \mathbf{x'}\|^3 - 3R \|\mathbf{x} - \mathbf{x'}\|^2 + R^3
\end{equation}
where $l$ is the kernel \textit{length scale} and $R$ is the maximum distance between datapoints.

\section{GP with gradient information}
In a GP regression problem, it is possible to provide for each $d$-dimensional point $\mathbf{x} \in \mathrm{R}^d$ both the observation $y$ and its derivative $\nabla y$. Let $\mathbf{y}^+ \in \mathrm{R}^{N(d+1)}$ be the vector of all observations and their derivatives organized as:
\begin{align}
	\mathbf{y}^+ = 
	\begin{bmatrix}
		\mathbf{y} \\
		\nabla \mathbf{y} 
	\end{bmatrix},
	&&
	\nabla \mathbf{y} =
	\begin{bmatrix}
		\nabla_{x_1} y_1 \\
		\vdots \\
		\nabla_{x_1} y_N \\
		\vdots \\
		\nabla_{x_d} y_1 \\
		\vdots \\
		\nabla_{x_d} y_N
	\end{bmatrix}_{Nd \times 1}
\end{align}
To describe the covariance between observations and their gradients, an augmented kernel $K^+_{(\mathbf{x}, \mathbf{x'})} \in \mathrm{R}^{N(d+1) \times N(d+1)}$ with gradients is used:
\begin{equation}
	K^+_{(\mathbf{x}, \mathbf{x'})} =	
	\begin{bmatrix}
		K_{(\mathbf{x}, \mathbf{x'})} && K_{(\mathbf{x}, \mathbf{x'})} \nabla_{\mathbf{x}'} ^\trsp \\
		\nabla_{\mathbf{x}} K_{(\mathbf{x}, \mathbf{x'})} && \nabla_{\mathbf{x}} K_{(\mathbf{x}, \mathbf{x'})} \nabla_{\mathbf{x}'} ^\trsp
	\end{bmatrix}
\end{equation}
where $\nabla_{\mathbf{x}} = \left[ \frac{\partial}{\partial x_1} \ldots \frac{\partial}{\partial x_d} \right]^\trsp$. \\
As a consequence, the GP regression problem can be rearranged from \autoref{eq:GPregr} as:
\begin{equation}\label{eq:GPgrad}
	\begin{aligned}
		\begin{bmatrix}
			\mathbf{\bar{f}_*} \\
			\nabla \mathbf{\bar{f}_*}
		\end{bmatrix}
		&= \mathbf{k}_*^+[\mathbf{K^+} + \sigma_{N(d+1)}^2 \mathrm{I}]^{-1}\mathbf{y^+}\\   
		\begin{bmatrix}
			\cov(\mathbf{f}_*) \\
			\nabla \cov(\mathbf{f}_*)
		\end{bmatrix} 
		&= \mathbf{k}_{**}^+ - \mathbf{k}_*^+[\mathbf{K}^+ + \sigma_{N(d+1)}^2 \mathrm{I}]^{-1} \left( \mathbf{k}_*^+ \right) ^\trsp 
	\end{aligned}
\end{equation}

For the 2D case, the augmented covariance matrix turns out to be:
\begin{equation}\label{eq:augm2Dkernel}
	K^+_{(\mathbf{x}, \mathbf{x'})} =
	\begin{bmatrix}
		K_{(\mathbf{x}, \mathbf{x'})} && \frac{\partial K_{(\mathbf{x}, \mathbf{x'})}}{\partial x_1'} && \frac{\partial K_{(\mathbf{x}, \mathbf{x'})}}{\partial x_2'} \\
		\frac{\partial K_{(\mathbf{x}, \mathbf{x'})}}{\partial x_1} && \frac{\partial^2 K_{(\mathbf{x}, \mathbf{x'})}}{\partial x_1 \partial x_1'} && \frac{\partial^2 K_{(\mathbf{x}, \mathbf{x'})}}{\partial x_1 \partial x_2'} \\
		\frac{\partial K_{(\mathbf{x}, \mathbf{x'})}}{\partial x_2} && \frac{\partial^2 K_{(\mathbf{x}, \mathbf{x'})}}{\partial x_2 \partial x_1'} && \frac{\partial^2 K_{(\mathbf{x}, \mathbf{x'})}}{\partial x_2 \partial x_2'}
	\end{bmatrix}
\end{equation}
%and the observation vector:
\subsection{Kernel derivatives}
A list of kernel partial derivatives is provided.
\subsubsection{Squared Exponential covariance function}
The Squared Exponential kernel is presented in \autoref{eq:sekernel}. Differentiation leads to the fllowing expressions:
\begin{align}
	\frac{\partial K_{SE(\mathbf{x}, \mathbf{x'})}}{\partial x_i} &= -\frac{x_i - x_i'}{l^2} K_{SE(\mathbf{x}, \mathbf{x'})} \\
	\frac{\partial^2 K_{SE(\mathbf{x}, \mathbf{x'})}}{\partial x_i \partial x_j} &= \left( \frac{1}{l^2} \delta_{ij} - \frac{(x_i - x_i')(x_j - x_j')}{l^4} \right) K_{SE(\mathbf{x}, \mathbf{x'})}
\end{align}

\subsubsection{Thin Plate covariance function}
Deriving \autoref{eq:tpkernel} with respect to each component of $\mathbf{x}$, one obtains:
\small
\begin{align}
	\frac{\partial K_{TP(\mathbf{x}, \mathbf{x'})}}{\partial x_i} &= 6(x_i - x_i') \left( \|\mathbf{x} - \mathbf{x'}\| - R \right) \\
	\frac{\partial K_{TP(\mathbf{x}, \mathbf{x'})}}{\partial x_i \partial x_j} &= -6 \left( \frac{(x_i - x_i')(x_j - x_j')}{\|\mathbf{x} - \mathbf{x'}\|} + \delta_{ij}(\|\mathbf{x} - \mathbf{x'}\| - R) \right)
\end{align}
\normalsize

\subsubsection{Matérn 5/2 covariance function}
Setting $d = \|\mathbf{x} - \mathbf{x'}\|$, the \textit{Matérn} $\frac{5}{2}$ covariance function is of the form: 
\begin{equation}\label{eq:matern}
	K_{M(\mathbf{x}, \mathbf{x'})} = \left( 1 + \frac{\sqrt{5}}{l} d + \frac{5}{3l^2} d^2 \right) \exp \left( -\frac{\sqrt{5}}{l} d \right)
\end{equation}
Its partial derivatives are:
\small
\begin{align}
	\begin{split}
		\frac{\partial K_{M(\mathbf{x}, \mathbf{x'})}}{\partial x_i} ={}& \exp \left( -\frac{\sqrt{5}}{l} d \right) \cdot \\
		& \cdot \left[ \frac{5}{l^2}(x_i - x_i')\left(\left(\frac{2}{3} - \frac{\sqrt{5}}{3l}\right) d - 1 \right) \right]
	\end{split}
	 \\
	 \begin{split}
	 	\frac{\partial^2  K_{M(\mathbf{x}, \mathbf{x'})}}{\partial x_i \partial x_j} ={}& \left( \frac{\sqrt{5}}{l} \frac{x_j - x_j'}{d} \right) \frac{\partial K_{M(\mathbf{x}, \mathbf{x'})}}{\partial x_i} + \\
	 	& \hspace{-40pt} + \exp \left( -\frac{\sqrt{5}}{l} d \right) \Bigg[ \Bigg. \frac{(x_i - x_i')(x_j - x_j')}{d} \left( \frac{5 \sqrt{5}}{3l^3} - \frac{10}{3l^2} \right) + \\
	 	& \hspace{-40pt} + \delta_{ij} \left( \frac{5 \sqrt{5}}{3l^3}d - \frac{10}{3l^2}d + \frac{5}{l^2} \right) \Bigg. \Bigg]
	 \end{split}	
\end{align}
\normalsize

\section{Fast Approximate GPIS}	
The Fast Approximate GPIS algorithm relies on the Kernel Decomposition algorithm presented in \autoref{kernelDec}, which allows to rewrite the GP problem with an approximated -- but smaller -- kernel matrix. The advantage is that the kernel matrix inversion is faster.\\
Regrouping the terms in \autoref{eq:mercerLim}, one obtains:
\begin{equation}\label{eq:kernelDec}
	K_{(\mathbf{x}, \mathbf{x'})} \approx \mathbf{\Phi}_{(\mathbf{x})} \Lambda \mathbf{\Phi}_{(\mathbf{x'})}^\trsp
\end{equation}
where:
\begin{align}
	\mathbf{\Phi} &=
	\begin{bmatrix}
		\varphi_1(\mathbf{x_1}) \hspace{-5mm} && \ldots && \hspace{-5mm} \varphi_m(\mathbf{x_1}) \\
		\vdots && && \vdots \\
		\varphi_1(\mathbf{x_N}) \hspace{-5mm} && \ldots && \hspace{-5mm} \varphi_m(\mathbf{x_N})
	\end{bmatrix}
	&&
	\Lambda = 
	\begin{bmatrix}
		\lambda_1 \hspace{-5mm}&& && \\
		&& \ddots && \\
		&& && \hspace{-5mm} \lambda_m
	\end{bmatrix},
\end{align}
with $N$ the number of datapoints, $m$ the number of the considered eigenvalues. In this implementation, $\varphi$ is defined in \autoref{eq:SEeigfun}.
Substituting \autoref{eq:kernelDec} in \autoref{eq:GPregr} and rewriting the terms using the \textit{binomial inverse theorem}, the GP regression problem can be approximated as:
\begin{equation}
	\begin{aligned}
		\mathbf{\bar{f}_*} &\approx \mathbf{W}\mathbf{y}\\    
		\cov(\mathbf{f}_*) &\approx \mathbf{\Phi}_{(\mathbf{x}_*)} \Lambda \mathbf{\Phi}_{(\mathbf{x}_*)}^\trsp - \mathbf{W} \mathbf{\Phi}_{(\mathbf{x})} \Lambda \mathbf{\Phi}_{(\mathbf{x}_*)}^\trsp
	\end{aligned}
\end{equation}
in which
\begin{align*}
	\mathbf{W} &= \mathbf{\Phi}_{(\mathbf{x}_*)} \Lambda \mathbf{\Phi}_{(\mathbf{x})}^\trsp \left( \Sigma_N^{-1} - \Sigma_N^{-1} \mathbf{\Phi}_{(\mathbf{x})} \mathbf{\bar{\Lambda}}^{-1} \mathbf{\Phi}_{(\mathbf{x})}^\trsp \Sigma_N^{-1} \right) \\
	\mathbf{\bar{\Lambda}} &= \Lambda^{-1} + \mathbf{\Phi}_{(\mathbf{x})}^\trsp \Sigma_N^{-1} \mathbf{\Phi}_{(\mathbf{x})}
\end{align*}
Note that the regression problem needs the inversion of $\mathbf{\bar{\Lambda}}$, which is a $m \times m$ matrix. Since $m << N$, the matrix inversion is computationally less demanding with respect to the classical GP problem.
\section{Logarithmic GPIS}
Logarithmic GPIS allows accurate description of the distance from objects. In this implementation, also the gradient information is considered in the LogGPIS problem. Contrarily to what happens for the standard GPIS problem presented in \autoref{GPIS}, in LogGPIS only the information of the obstacle border and its gradient are needed. A train dataset $ \{ (\mathbf{x}_i, y_i) \}_{i=1}^N$, with $\mathbf{x}_i \in \mathbf{X}$ and $y_i \in \mathbf{y}$ and a test dataset $\{ (\mathbf{x}_{*i}, y_{*i}) \}_{i=1}^M$, with $\mathbf{x}_{*i} \in \mathbf{X_*}$ are given. For the 2D case, the observation vector at each $\mathbf{x}_i = [ x_1 \hspace{2mm} x_2]$ is defined as:
\begin{equation}
	\mathbf{y} = 
	\begin{bmatrix}
		1 \hspace{-5mm} && \ldots && \hspace{-5mm} 1
	\end{bmatrix}^\trsp_{N \times 1}
\end{equation}
and the augmented observation vector is:
\begin{align*}
	\mathbf{y}^+ &= 
	\begin{bmatrix}
		\mathbf{y}	 \\
		\mathbf{\nabla y}
	\end{bmatrix}_{3N \times 1}
	&&
	\mathbf{\nabla y} = 
	\begin{bmatrix}
		\frac{\partial y_1}{\partial x_1} \\
		\vdots \\
		\frac{\partial y_N}{\partial x_1} \vspace{2mm}\\
		\frac{\partial y_1}{\partial x_2} \\
		\vdots \\
		\frac{\partial y_N}{\partial x_2} \\
	\end{bmatrix}_{2N \times 1}
\end{align*}
Matérn 5/2 kernel (\autoref{eq:matern}) is used, as it shows higher distance accuracy and it is two-times differentiable. The augmented kernel $K^+_{(\mathbf{x}, \mathbf{x'})}$ has the same structure as \autoref{eq:augm2Dkernel}.\\
To obtain the distance for every test points in the test dataset $\mathbf{X_*}$, the following transfomations are needed:
\begin{equation}
	\begin{aligned}
		\mathbf{\bar{d}_*} = -\frac{\ln(\mathbf{\bar{f}_*})}{\sqrt{5}/l}, &&
		\nabla \mathbf{\bar{d}_*} = - \frac{1}{\sqrt{5}/l} \frac{\nabla \mathbf{\bar{f}_*}}{\mathbf{\bar{f}_*}}
	\end{aligned}
\end{equation}
where $\mathbf{\bar{f}_*}$ and $\nabla \mathbf{\bar{f}_*}$ are defined in \autoref{eq:GPgrad}.
The covariance turns out to be:
\begin{equation}
	\cov(\mathbf{d}_*) = \frac{l}{\sqrt{5} \hspace{1mm} \mathbf{\bar{f}_*}} \cov(\mathbf{f}_*) \frac{l}{\sqrt{5} \hspace{1mm} \mathbf{\bar{f}_*}}^\trsp
\end{equation}

%\section{Recursive GPIS}
\end{document}
